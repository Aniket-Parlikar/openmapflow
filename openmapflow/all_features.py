from cropharvest.utils import memoized
from pathlib import Path
from typing import List
import pandas as pd
import pickle
import tarfile
from openmapflow.utils import try_txt_read
from openmapflow.config import PROJECT_ROOT, DataPaths as dp
from openmapflow.labeled_dataset import LabeledDataset


@memoized
def load_all_features_as_df() -> pd.DataFrame:
    duplicates_data = try_txt_read(PROJECT_ROOT / dp.DUPLICATES)
    features = []
    files = list((PROJECT_ROOT / dp.FEATURES).glob("*.pkl"))
    print("------------------------------")
    print("Loading all features...")
    non_duplicated_files = []
    for p in files:
        if p.stem not in duplicates_data:
            non_duplicated_files.append(p)
            with p.open("rb") as f:
                features.append(pickle.load(f))
    df = pd.DataFrame([feat.__dict__ for feat in features])
    df["filename"] = non_duplicated_files
    return df


def check_features_df_empty(df: pd.DataFrame) -> str:
    """
    Some exported tif data may have nan values
    """
    if len(df) == 0:
        return "No features found"
    empties = df[df["labelled_array"].isnull()]
    num_empty = len(empties)
    if num_empty > 0:
        return f"\u2716 Found {num_empty} empty features"
    else:
        return "\u2714 Found no empty features"


def check_features_df_duplicates(df: pd.DataFrame) -> str:
    """
    Duplicates can occur when not all tifs have been downloaded
    and different labels are matched to same tif
    """
    if len(df) == 0:
        return "No features found"
    cols_to_check = ["instance_lon", "instance_lat", "source_file"]
    duplicates = df[df.duplicated(subset=cols_to_check)]
    num_dupes = len(duplicates)
    if num_dupes > 0:
        return f"\u2716 Found {num_dupes} duplicates"
    else:
        return "\u2714 No duplicates found"


def create_all_features(datasets: List[LabeledDataset]):
    report = "DATASET REPORT (autogenerated, do not edit directly)"
    for d in datasets:
        text = d.create_features()
        report += "\n\n" + text

    df = load_all_features_as_df()
    empty_text = check_features_df_empty(df)
    duplicates_text = check_features_df_duplicates(df)
    print(empty_text)
    print(duplicates_text)
    report += "\n\nAll data:\n" + empty_text + "\n" + duplicates_text

    with (PROJECT_ROOT / dp.DATASETS).open("w") as f:
        f.write(report)

    # Compress features for faster CI/CD
    print("Compressing features...")
    with tarfile.open(PROJECT_ROOT / dp.COMPRESSED_FEATURES, "w:gz") as tar:
        tar.add(PROJECT_ROOT / dp.FEATURES, arcname=Path(dp.FEATURES).name)
