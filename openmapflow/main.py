from typing import List, Optional
import subprocess
import tarfile

from .config import relative_paths, full_paths
from .all_features import AllFeatures
from .dataset import LabeledDataset

training_data_path_keys = ["raw", "processed", "compressed_features.tar.gz"]


def _dvc(command: str, path_key: Optional[str] = None):
    if command not in ["pull", "commit", "push"]:
        raise ValueError(f"Unknown command: {command}")

    if path_key not in ["raw", "processed", "compressed_features.tar.gz", "models"]:
        raise ValueError(f"Unknown path_key: {path_key}")

    if path_key is None:
        return subprocess.run(["dvc", command], check=True, capture_output=True)

    dvc_dir = full_paths[path_key]
    if not dvc_dir.exists():
        subprocess.run(
            ["dvc", command, relative_paths[path_key]], check=True, capture_output=True
        )
        if not dvc_dir.exists():
            raise FileExistsError(f"{str(dvc_dir)} was not found.")
        if not any(dvc_dir.iterdir()):
            raise FileExistsError(f"{str(dvc_dir)} should not be empty.")


def dvc_pull(path_key: str):
    _dvc("pull", path_key)


def get_training_data():
    for path_key in training_data_path_keys:
        _dvc("pull", path_key)
    tar = tarfile.open(relative_paths["compressed_features.tar.gz"], "r:gz")
    tar.extractall()
    tar.close()


def push_new_training_data():
    for path_key in training_data_path_keys:
        _dvc("commit", path_key)
    _dvc("push")


def create_features(datasets: List[LabeledDataset]):
    get_training_data()

    report = "DATASET REPORT (autogenerated, do not edit directly)"
    for d in datasets:
        text = d.create_features()
        report += "\n\n" + text

    all_features = AllFeatures()
    empty_text = all_features.check_empty()
    print(empty_text)
    duplicates_text = all_features.check_duplicates()
    print(duplicates_text)
    report += "\n\nAll data:\n" + empty_text + "\n" + duplicates_text

    with full_paths["datasets"].open("w") as f:
        f.write(report)

    # Compress features for faster CI/CD
    print("Compressing features...")
    with tarfile.open(full_paths["datasets"], "w:gz") as tar:
        tar.add(full_paths["datasets"], arcname="features")
