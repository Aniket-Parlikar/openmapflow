{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBYSuraxoKJy"
      },
      "source": [
        "# Model training üèã\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nasaharvest/openmapflow/blob/main/openmapflow/notebooks/train.ipynb)\n",
        "\n",
        "Notebook for training OpenMapFlow models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdI-wLrbxHZn"
      },
      "source": [
        "# 1. Setup\n",
        "\n",
        "If you don't already have one, obtain a Github Personal Access Token using the steps [here](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token). Save this token somewhere private."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3otirx9-y6M"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import auth\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    \n",
        "if IN_COLAB:\n",
        "    github_url = input(\"Github HTTPS URL: \")\n",
        "    email = input(\"Github email: \")\n",
        "    username = input(\"Github username: \")\n",
        "\n",
        "    !git config --global user.email $username\n",
        "    !git config --global user.name $email\n",
        "\n",
        "    from getpass import getpass\n",
        "    token = getpass('Github Personal Access Token:')\n",
        "\n",
        "    !git clone {github_url.replace(\"https://\", f\"https://{username}:{token}@\")}\n",
        "\n",
        "    # Temporarily install from Github\n",
        "    !pip install git+https://ivanzvonkov:$token@github.com/nasaharvest/openmapflow.git -q\n",
        "else:\n",
        "    print(\"Running notebook outside Google Colab. Assuming in local repository.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbW89ktDl6ku"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "openmapflow_yaml_path = input(\"Path to openmapflow.yaml: \")\n",
        "%cd {Path(openmapflow_yaml_path).parent}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzZn9b3f2ySY"
      },
      "outputs": [],
      "source": [
        "!pip install cmocean torch wandb tsai earthengine-api google-auth -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWoGz94avN0w"
      },
      "outputs": [],
      "source": [
        "from cropharvest.bands import DYNAMIC_BANDS\n",
        "from cropharvest.eo import EarthEngineExporter\n",
        "from cropharvest.inference import Inference\n",
        "from cropharvest.countries import BBox\n",
        "from google.cloud import storage\n",
        "from datetime import date\n",
        "from pathlib import Path\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix, \n",
        "    ConfusionMatrixDisplay\n",
        ")\n",
        "\n",
        "import cmocean\n",
        "import ee\n",
        "import google\n",
        "import ipywidgets as widgets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import rasterio as rio\n",
        "import pandas as pd\n",
        "import tempfile\n",
        "import torch\n",
        "import wandb\n",
        "import warnings\n",
        "import yaml\n",
        "\n",
        "from openmapflow.config import PROJECT_ROOT, DataPaths, BucketNames, PROJECT, GCLOUD_PROJECT_ID\n",
        "from openmapflow.pytorch_dataset import PyTorchDataset\n",
        "from openmapflow.constants import SUBSET\n",
        "\n",
        "from datasets import datasets\n",
        "\n",
        "warnings.simplefilter(\"ignore\", UserWarning) # TorchScript throws excessive warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEusgSrCqxaz"
      },
      "source": [
        "# 2. Download latest data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls-7sN9Hoew6"
      },
      "outputs": [],
      "source": [
        "for p in tqdm([DataPaths.MODELS, DataPaths.PROCESSED_LABELS, DataPaths.COMPRESSED_FEATURES]):\n",
        "    !dvc pull {p} -q\n",
        "\n",
        "!tar -xzf {DataPaths.COMPRESSED_FEATURES} -C data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeANCDe2uJcX"
      },
      "outputs": [],
      "source": [
        "# Currently available models\n",
        "sorted([p.stem for p in (PROJECT_ROOT / DataPaths.MODELS).glob('*.pt')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAD4tO5k7nO5"
      },
      "outputs": [],
      "source": [
        "# Available datasets for training and evaluation\n",
        "!cat {PROJECT_ROOT / DataPaths.DATASETS}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gietI36Bykse"
      },
      "source": [
        "# 3. Train model\n",
        "<img src=\"https://storage.googleapis.com/harvest-public-assets/openmapflow/train_model.png\" width=80%/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P13Z7qUQfMJ"
      },
      "source": [
        "### 3.1 Import model\n",
        "Any PyTorch based model that can take sequence data as input will work here.\n",
        "Example uses a PyTorch model from [tsai](https://github.com/timeseriesAI/tsai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEUjElwSELhF"
      },
      "outputs": [],
      "source": [
        "from tsai.models.TransformerModel import TransformerModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAYc2qnpQXu1"
      },
      "source": [
        "### 3.2 Setup training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lBX3COiquUPN"
      },
      "outputs": [],
      "source": [
        "# ------------ Dataloaders -------------------------------------\n",
        "df = pd.concat([d.load_labels() for d in datasets])\n",
        "train_data = PyTorchDataset(\n",
        "    df=df[df[SUBSET] == \"training\"], \n",
        "    start_month=\"February\", \n",
        "    subset=\"training\", \n",
        "    upsample=True\n",
        ")\n",
        "val_data = PyTorchDataset(\n",
        "    df=df[df[SUBSET] == \"validation\"], \n",
        "    start_month=\"February\", \n",
        "    subset=\"validation\", \n",
        ")\n",
        "test_data = PyTorchDataset(\n",
        "    df=df[df[SUBSET] == \"testing\"], \n",
        "    start_month=\"February\", \n",
        "    subset=\"testing\", \n",
        ")\n",
        "\n",
        "batch_size = 128\n",
        "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=False) \n",
        "\n",
        "num_timesteps, num_bands = train_data[0][0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y5P6Lc10icC"
      },
      "outputs": [],
      "source": [
        "# ------------ Model -----------------------------------------\n",
        "class Model(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.model = TransformerModel(c_in=num_bands, c_out=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.model(x.transpose(2,1)).squeeze(dim=1)\n",
        "    x = torch.sigmoid(x)\n",
        "    return x\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Model().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ikl6dwDx0gHk"
      },
      "outputs": [],
      "source": [
        "# ------------ Model parameters -------------------------------------\n",
        "lr = 0.0001\n",
        "params_to_update = model.parameters()\n",
        "optimizer = torch.optim.SGD(params_to_update, lr=lr, momentum=0.9)\n",
        "criterion = torch.nn.BCELoss()\n",
        "num_epochs = 50\n",
        "model_input = widgets.Text(description='Model name ')\n",
        "model_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vag_syuI_7LC"
      },
      "source": [
        "### 3.3 Training loop\n",
        "Inspired by [PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4CEAVqR6Vi4"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "#%%wandb\n",
        "\n",
        "model_name = model_input.value\n",
        "assert model_name != \"\", \"Please input model name\"\n",
        "\n",
        "training_config = {\n",
        "  \"model_name\": model_name,\n",
        "  \"model\": model.__class__,\n",
        "  \"batch_size\": batch_size,\n",
        "  \"num_epochs\": num_epochs,\n",
        "  \"lr\": lr,\n",
        "  \"optimizer\": optimizer.__class__.__name__,\n",
        "  \"loss\": criterion.__class__.__name__,\n",
        "  **train_data.dataset_info, \n",
        "  **val_data.dataset_info\n",
        "}\n",
        "run = wandb.init(project=PROJECT, config=training_config)\n",
        "\n",
        "lowest_validation_loss = None\n",
        "train_batches = 1 + len(train_data) // batch_size\n",
        "val_batches = 1 + len(val_data) // batch_size\n",
        "\n",
        "for epoch in tqdm(range(num_epochs), total=num_epochs):  \n",
        "\n",
        "    # ------------------------ Training ----------------------------------------\n",
        "    total_train_loss = 0.0\n",
        "    model.train()\n",
        "    for x in tqdm(train_dataloader, total=train_batches, desc=\"Train\", leave=False):\n",
        "      inputs, labels = x[0].to(device), x[1].to(device)\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Get model outputs and calculate loss\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_train_loss += (loss.item() * len(inputs))\n",
        "\n",
        "    # ------------------------ Validation --------------------------------------\n",
        "    total_val_loss = 0.0\n",
        "    y_true = []\n",
        "    y_score = []\n",
        "    y_pred = []\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "      for x in tqdm(val_dataloader, total=val_batches, desc=\"Validate\", leave=False):\n",
        "        inputs, labels = x[0].to(device), x[1].to(device)\n",
        "\n",
        "        # Get model outputs and calculate loss\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        total_val_loss += (loss.item() * len(inputs))\n",
        "\n",
        "        y_true += labels.tolist()\n",
        "        y_score += outputs.tolist()\n",
        "        y_pred += (outputs > 0.5).long().tolist()\n",
        "    \n",
        "\n",
        "    # ------------------------ Metrics + Logging -------------------------------\n",
        "    train_loss = total_train_loss / len(train_data)\n",
        "    val_loss = total_val_loss / len(val_data)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    ConfusionMatrixDisplay(cm, display_labels=[\"Negative\", \"Positive\"]).plot()\n",
        "    to_log = {\n",
        "      \"train_loss\": train_loss, \n",
        "      \"val_loss\":   val_loss, \n",
        "      \"epoch\":      epoch,\n",
        "      \"accuracy\":   accuracy_score(y_true, y_pred),\n",
        "      \"f1\":         f1_score(y_true, y_pred),\n",
        "      \"precision\":  precision_score(y_true, y_pred),\n",
        "      \"recall\":     recall_score(y_true, y_pred),   \n",
        "      \"roc_auc\":    roc_auc_score(y_true, y_score),\n",
        "      \"confusion_matrix\": wandb.Image(plt)\n",
        "    }\n",
        "    wandb.log(to_log)\n",
        "    plt.close(\"all\")\n",
        "\n",
        "    # ------------------------ Model saving --------------------------\n",
        "    if lowest_validation_loss is None or val_loss < lowest_validation_loss:\n",
        "      lowest_validation_loss = val_loss\n",
        "      sm = torch.jit.script(model)\n",
        "      model_path = PROJECT_ROOT / DataPaths.MODELS / f\"{model_name}.pt\"\n",
        "      if model_path.exists():\n",
        "          model_path.unlink()\n",
        "      sm.save(str(model_path))\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqRGvmJBwOcv"
      },
      "outputs": [],
      "source": [
        "# Newly available models\n",
        "sorted([p.stem for p in (PROJECT_ROOT / DataPaths.MODELS).glob('*.pt')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lm6__TQZoq0"
      },
      "source": [
        "### 3.4 Record test metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCWZ7g72Zt21"
      },
      "outputs": [],
      "source": [
        "model_pt = torch.jit.load(model_path)\n",
        "model_pt.eval()\n",
        "\n",
        "test_batches = 1 + len(test_data) // batch_size\n",
        "y_true = []\n",
        "y_score = []\n",
        "y_pred = []\n",
        "model.eval() \n",
        "with torch.no_grad():\n",
        "  for x in tqdm(test_dataloader, total=test_batches, desc=\"Testing\", leave=False):\n",
        "    inputs, labels = x[0].to(device), x[1].to(device)\n",
        "\n",
        "    # Get model outputs and calculate loss\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    total_val_loss += (loss.item() * len(inputs))\n",
        "\n",
        "    y_true += labels.tolist()\n",
        "    y_score += outputs.tolist()\n",
        "    y_pred += (outputs > 0.5).long().tolist()\n",
        "\n",
        "metrics = {\n",
        "  \"accuracy\":   accuracy_score(y_true, y_pred),\n",
        "  \"f1\":         f1_score(y_true, y_pred),\n",
        "  \"precision\":  precision_score(y_true, y_pred),\n",
        "  \"recall\":     recall_score(y_true, y_pred),   \n",
        "  \"roc_auc\":    roc_auc_score(y_true, y_score),\n",
        "}\n",
        "metrics = {k: round(float(v), 4) for k,v in metrics.items()}\n",
        "\n",
        "all_metrics = {}\n",
        "if (PROJECT_ROOT / DataPaths.METRICS).exists():\n",
        "  with (PROJECT_ROOT / DataPaths.METRICS).open() as f:\n",
        "      all_metrics = yaml.safe_load(f)\n",
        "\n",
        "all_metrics[model_name] = {\"params\": run.url, \"test_metrics\": metrics, **test_data.dataset_info}\n",
        "\n",
        "with open((PROJECT_ROOT / DataPaths.METRICS), 'w') as f:\n",
        "    yaml.dump(all_metrics, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XG94Q3lAzmyu"
      },
      "source": [
        "# 4. Pushing the model to the repository"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fbv1fwFNzrnS"
      },
      "outputs": [],
      "source": [
        "!dvc commit {DataPaths.MODELS} -q -f\n",
        "!dvc push -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EywOpWv8JDV"
      },
      "outputs": [],
      "source": [
        "# Push changes to github\n",
        "!git checkout -b'{model_name}'\n",
        "!git add .\n",
        "!git commit -m 'Trained new: {model_name}'\n",
        "!git push --set-upstream origin \"{model_name}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YENWoPX_1AJC"
      },
      "source": [
        "Create a Pull Request so the model can be merged into the main branch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1U7eVuRy9yj-"
      },
      "source": [
        "# 5. [OPTIONAL] Create small map with model\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/harvest-public-assets/openmapflow/basic_inference.png\" width=\"80%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2q9N4kwiJT6"
      },
      "source": [
        "### 5.1 Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nys8E_hBAEGb"
      },
      "outputs": [],
      "source": [
        "bbox_name = \"Togo_2019_demo\"\n",
        "bbox = BBox(min_lat=6.31, max_lat=6.34, min_lon=1.70, max_lon=1.74)\n",
        "start_date= date(2019, 2, 1)\n",
        "end_date= date(2020,2,1)\n",
        "prefix = f\"{bbox_name}_{start_date}_{end_date}\"\n",
        "print(bbox.url)\n",
        "\n",
        "temp_dir = tempfile.gettempdir()\n",
        "\n",
        "print(\"Logging into Google Cloud\")\n",
        "auth.authenticate_user()\n",
        "print(\"Logging into Earth Engine\")\n",
        "SCOPES = ['https://www.googleapis.com/auth/cloud-platform', 'https://www.googleapis.com/auth/earthengine']\n",
        "CREDENTIALS, project_id = google.auth.default(default_scopes=SCOPES)\n",
        "ee.Initialize(CREDENTIALS, project=GCLOUD_PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFojbqZZiOg6"
      },
      "source": [
        "### 5.2 Download earth observation data for entire region (bbox)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCjiMASKIsWB"
      },
      "outputs": [],
      "source": [
        "client = storage.Client()\n",
        "cloud_tif_list_iterator = client.list_blobs(BucketNames.LABELED_TIFS, prefix=prefix)\n",
        "cloud_tif_list = [\n",
        "    blob.name\n",
        "    for blob in tqdm(cloud_tif_list_iterator, desc=\"Loading tifs already on Google Cloud\")\n",
        "]\n",
        "\n",
        "if len(cloud_tif_list) == 0:\n",
        "  EarthEngineExporter(check_ee=False, check_gcp=False, dest_bucket=BucketNames.LABELED_TIFS).export_for_bbox(    \n",
        "    bbox=bbox,\n",
        "    bbox_name=bbox_name,\n",
        "    start_date=date(2019, 2, 1),\n",
        "    end_date=date(2020,2,1),\n",
        "    metres_per_polygon=50000,\n",
        "    file_dimensions=256\n",
        "  )\n",
        "  print(\"Earth observation data is being exported, progress: https://code.earthengine.google.com/tasks\")\n",
        "else:\n",
        "  bucket = storage.Client().bucket(BucketNames.LABELED_TIFS)\n",
        "  local_tif_paths = []\n",
        "  for gs_path in tqdm(cloud_tif_list, desc=\"Downloading tifs\"):\n",
        "    local_path = Path(f\"{temp_dir}/{gs_path.replace('/', '_')}\")\n",
        "    if not local_path.exists():\n",
        "      bucket.blob(gs_path).download_to_filename(local_path)\n",
        "    local_tif_paths.append(local_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRFRu-q_iY2p"
      },
      "source": [
        "### 5.3 Make predictions for each pixel in the earth observation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jy5S0UJmMiDv"
      },
      "outputs": [],
      "source": [
        "inference = Inference(model=model, normalizing_dict=None, device=device, batch_size=batch_size)\n",
        "local_pred_paths = []\n",
        "for local_tif_path in tqdm(local_tif_paths, desc=\"Making predictions\"):\n",
        "  local_pred_path = Path(f\"{temp_dir}/pred_{local_tif_path.stem}.nc\")\n",
        "  inference.run(\n",
        "      local_path=local_tif_path, \n",
        "      start_date=start_date, \n",
        "      dest_path=local_pred_path\n",
        "  )\n",
        "  local_pred_paths.append(local_pred_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdMBEvTimEj"
      },
      "source": [
        "### 5.4 Merge pixel predictions into single map\n",
        "\n",
        "<img src=\"https://storage.googleapis.com/harvest-public-assets/openmapflow/merging_predictions.png\" width=\"60%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVjXSffaN02f"
      },
      "outputs": [],
      "source": [
        "def merge_tifs(full_prefix):\n",
        "  vrt_in_file = f\"{full_prefix}*\"\n",
        "  vrt_out_file = f\"{full_prefix}.vrt\"\n",
        "  merged_file = f\"{full_prefix}.tif\"\n",
        "  !gdalbuildvrt {vrt_out_file} {vrt_in_file}\n",
        "  !gdal_translate -a_srs EPSG:4326 -of GTiff {vrt_out_file} {merged_file}\n",
        "  return merged_file\n",
        "\n",
        "merged_eo_file = merge_tifs(full_prefix=f\"{temp_dir}/{prefix}\")\n",
        "merged_pred_file = merge_tifs(full_prefix=f\"{temp_dir}/pred_{prefix}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJhp96V4iu_t"
      },
      "source": [
        "### 5.5 Visualize earth observation data and predictions map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBLKrprQZSb7"
      },
      "outputs": [],
      "source": [
        "def normalize(array):\n",
        "    array_min, array_max = array.min(), array.max()*0.6\n",
        "    return ((array - array_min)/(array_max - array_min))\n",
        "\n",
        "month = 2\n",
        "rgb_indexes = [DYNAMIC_BANDS.index(b) for b in [\"B4\", \"B3\", \"B2\"]]\n",
        "colors = [merged_eo_file.read(i + month*len(DYNAMIC_BANDS)) for i in rgb_indexes]\n",
        "normalized_colors = [normalize(c) for c in colors]\n",
        "rgb = np.dstack(normalized_colors)\n",
        "plt.title(\"Earth Observation data for one month\")\n",
        "plt.axis('off')\n",
        "plt.imshow(rgb);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhoY-ujuVQYU"
      },
      "outputs": [],
      "source": [
        "predictions_map = rio.open(merged_pred_file)\n",
        "plt.title(\"Model predicted map\")\n",
        "plt.axis('off')\n",
        "rio.plot.show(predictions_map, cmap=cmocean.cm.speed);"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
